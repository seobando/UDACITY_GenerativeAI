{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seobando/UDACITY_GenerativeAI/blob/main/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q \"datasets==2.15.0\""
      ],
      "metadata": {
        "id": "vOVT8Oa4Jkka"
      },
      "id": "vOVT8Oa4Jkka",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U transformer"
      ],
      "metadata": {
        "id": "-iJP8JIR8rQR"
      },
      "id": "-iJP8JIR8rQR",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install peft"
      ],
      "metadata": {
        "id": "pnN68XiM8-eF"
      },
      "id": "pnN68XiM8-eF",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique: LoRA\n",
        "* Model: gpt2\n",
        "* Evaluation approach: accuracy evaluation\n",
        "* Fine-tuning dataset: imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "6e5e5293",
      "metadata": {
        "id": "6e5e5293"
      },
      "outputs": [],
      "source": [
        "dataset_name= \"imdb\"\n",
        "model_name = \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "049e9138",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049e9138",
        "outputId": "491bed84-648f-4b4d-d5bb-71736c5bbd26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['text', 'label'],\n",
              "     num_rows: 800\n",
              " }),\n",
              " 'test': Dataset({\n",
              "     features: ['text', 'label'],\n",
              "     num_rows: 800\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "# Import the datasets and transformers packages\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the train and test splits of the imdb dataset\n",
        "splits = [\"train\", \"test\"]\n",
        "ds = {split: ds for split, ds in zip(splits, load_dataset(dataset_name, split=splits))}\n",
        "\n",
        "# Thin out the dataset to make it run faster for this example\n",
        "for split in splits:\n",
        "    ds[split] = ds[split].shuffle(seed=42).select(range(800))\n",
        "\n",
        "# Show the dataset\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "ada3cfb5",
      "metadata": {
        "id": "ada3cfb5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the emotion dataset by returning tokenized examples.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KIeah2n0FLA2"
      },
      "id": "KIeah2n0FLA2"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "f95e5b0d",
      "metadata": {
        "id": "f95e5b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e76bf95514c2432bad7dada6a0a78805",
            "41ff3bc6da1f4345959cd2336864f0c4",
            "7e6eb305eda549149ad2783db2e0a61b",
            "a62e8e789714460bb0bbe0d956563d46",
            "915a9d2859f443e5959e606da90ee2b6",
            "5dfe9894df484ea0b28de06fb2e09a69",
            "39890a24aa444e44965de0f0a2d5a94d",
            "504b0e66feff4c9bbf8e3e543156b282",
            "1eac745512794550a21c87d8d91b0a99",
            "bc7d57ef258a4290887902e7ba083563",
            "c5092fc5d9d74555954cc8eb1504f2e5"
          ]
        },
        "outputId": "53195ead-e8d2-44ac-82cc-54c96cf3b1fa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e76bf95514c2432bad7dada6a0a78805"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_data = {}\n",
        "\n",
        "for split in splits:\n",
        "    tokenized_data[split] = ds[split].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "dd364a8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd364a8e",
        "outputId": "2bb13add-e465-47d6-ac73-f1b7e74c1414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
        "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "43806846",
      "metadata": {
        "scrolled": true,
        "id": "43806846"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "7cb59618",
      "metadata": {
        "id": "7cb59618"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "dfbfe58a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfbfe58a",
        "outputId": "86f67dba-858b-4ac3-a8d9-9488d658f9fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args= training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\"),\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "2f339ceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f339ceb",
        "outputId": "c2a78efa-a066-4cbe-a5ea-f60e223a7af6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "45571b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "45571b43",
        "outputId": "b83822cc-a2b6-4c82-a0c2-47ed5315b94f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 02:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.693027</td>\n",
              "      <td>0.602500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.8580416870117188, metrics={'train_runtime': 143.7887, 'train_samples_per_second': 5.564, 'train_steps_per_second': 1.391, 'total_flos': 418074800947200.0, 'train_loss': 0.8580416870117188, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "8fcf55eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8fcf55eb",
        "outputId": "041333f6-9b53-465a-93df-d8e5d5132927"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 01:09]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6930273175239563,\n",
              " 'eval_accuracy': 0.6025,\n",
              " 'eval_runtime': 69.7978,\n",
              " 'eval_samples_per_second': 11.462,\n",
              " 'eval_steps_per_second': 2.865,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "3c757d11",
      "metadata": {
        "id": "3c757d11"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"gpt2_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "997171e1",
      "metadata": {
        "id": "997171e1"
      },
      "outputs": [],
      "source": [
        "gpt_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "4e9c89f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e9c89f4",
        "outputId": "670184ba-1b78-4756-f164-3b190b739c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    inference_mode=False,\n",
        "    r=18,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    modules_to_save=[\"classifier\"],\n",
        "    bias='lora_only',\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(gpt_model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "b0f7721b",
      "metadata": {
        "id": "b0f7721b"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['c_attn', 'c_proj'],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    fan_in_fan_out=True,\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7d89bbfd",
      "metadata": {
        "id": "7d89bbfd"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    label_names=[\"labels\"],\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "95fa870e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95fa870e",
        "outputId": "258c8647-0fc2-464f-ae0b-41c30141d92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer = tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "d7ac357f",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "d7ac357f",
        "outputId": "675d3964-dbd1-454a-b999-4ce5a173032a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 01:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.654685</td>\n",
              "      <td>0.641250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.604139</td>\n",
              "      <td>0.692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.567063</td>\n",
              "      <td>0.708750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=300, training_loss=0.5451299540201823, metrics={'train_runtime': 71.2428, 'train_samples_per_second': 33.688, 'train_steps_per_second': 4.211, 'total_flos': 158275731456000.0, 'train_loss': 0.5451299540201823, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "f39a8442",
      "metadata": {
        "id": "f39a8442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "716e80fa-29a0-4137-e8bb-a02829a5c14d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5670626163482666,\n",
              " 'eval_accuracy': 0.70875,\n",
              " 'eval_runtime': 7.0179,\n",
              " 'eval_samples_per_second': 113.995,\n",
              " 'eval_steps_per_second': 14.249,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "ad1761bd",
      "metadata": {
        "id": "ad1761bd"
      },
      "outputs": [],
      "source": [
        "lora_model.save_pretrained(\"gpt-custom-lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "d7e9add6",
      "metadata": {
        "id": "d7e9add6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d65c6d-16bd-4ed9-fa6c-428c6d232aec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt-custom-lora-tokenize/tokenizer_config.json',\n",
              " 'gpt-custom-lora-tokenize/special_tokens_map.json',\n",
              " 'gpt-custom-lora-tokenize/vocab.json',\n",
              " 'gpt-custom-lora-tokenize/merges.txt',\n",
              " 'gpt-custom-lora-tokenize/added_tokens.json',\n",
              " 'gpt-custom-lora-tokenize/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"gpt-custom-lora-tokenize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "b70e99f5",
      "metadata": {
        "id": "b70e99f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "780b8673-8bd3-4061-ad2c-6194120ca4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "The current model class (GPT2ForSequenceClassification) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-1dfeeb490ba6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, my name is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_base_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_prompt_tuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;31m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_compatible_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0mexception_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The current model class (GPT2ForSequenceClassification) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
          ]
        }
      ],
      "source": [
        "from peft import AutoPeftModelForSequenceClassification\n",
        "\n",
        "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-custom-lora\")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"gpt-custom-lora-tokenizer\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
        "outputs = lora_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
        "print(tokenizer.batch_decode(outputs))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e76bf95514c2432bad7dada6a0a78805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41ff3bc6da1f4345959cd2336864f0c4",
              "IPY_MODEL_7e6eb305eda549149ad2783db2e0a61b",
              "IPY_MODEL_a62e8e789714460bb0bbe0d956563d46"
            ],
            "layout": "IPY_MODEL_915a9d2859f443e5959e606da90ee2b6"
          }
        },
        "41ff3bc6da1f4345959cd2336864f0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dfe9894df484ea0b28de06fb2e09a69",
            "placeholder": "​",
            "style": "IPY_MODEL_39890a24aa444e44965de0f0a2d5a94d",
            "value": "Map: 100%"
          }
        },
        "7e6eb305eda549149ad2783db2e0a61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504b0e66feff4c9bbf8e3e543156b282",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1eac745512794550a21c87d8d91b0a99",
            "value": 800
          }
        },
        "a62e8e789714460bb0bbe0d956563d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7d57ef258a4290887902e7ba083563",
            "placeholder": "​",
            "style": "IPY_MODEL_c5092fc5d9d74555954cc8eb1504f2e5",
            "value": " 800/800 [00:00&lt;00:00, 1075.62 examples/s]"
          }
        },
        "915a9d2859f443e5959e606da90ee2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dfe9894df484ea0b28de06fb2e09a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39890a24aa444e44965de0f0a2d5a94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "504b0e66feff4c9bbf8e3e543156b282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eac745512794550a21c87d8d91b0a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc7d57ef258a4290887902e7ba083563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5092fc5d9d74555954cc8eb1504f2e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}