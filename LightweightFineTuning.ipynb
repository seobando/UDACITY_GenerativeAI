{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seobando/UDACITY_GenerativeAI/blob/main/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"datasets==2.15.0\""
      ],
      "metadata": {
        "id": "vOVT8Oa4Jkka"
      },
      "id": "vOVT8Oa4Jkka",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U transformer"
      ],
      "metadata": {
        "id": "-iJP8JIR8rQR"
      },
      "id": "-iJP8JIR8rQR",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "pnN68XiM8-eF",
        "outputId": "08f4de99-a3c6-49da-9527-eaaaccb84cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pnN68XiM8-eF",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.28.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique: LoRA\n",
        "* Model: gpt2\n",
        "* Evaluation approach: accuracy evaluation\n",
        "* Fine-tuning dataset: imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6e5e5293",
      "metadata": {
        "id": "6e5e5293"
      },
      "outputs": [],
      "source": [
        "dataset_name= \"imdb\"\n",
        "model_name = \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "049e9138",
      "metadata": {
        "id": "049e9138",
        "outputId": "b4731418-2747-4dae-bf9a-85e72f7044ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['text', 'label'],\n",
              "     num_rows: 800\n",
              " }),\n",
              " 'test': Dataset({\n",
              "     features: ['text', 'label'],\n",
              "     num_rows: 800\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Import the datasets and transformers packages\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the train and test splits of the imdb dataset\n",
        "splits = [\"train\", \"test\"]\n",
        "ds = {split: ds for split, ds in zip(splits, load_dataset(dataset_name, split=splits))}\n",
        "\n",
        "# Thin out the dataset to make it run faster for this example\n",
        "for split in splits:\n",
        "    ds[split] = ds[split].shuffle(seed=42).select(range(800))\n",
        "\n",
        "# Show the dataset\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ada3cfb5",
      "metadata": {
        "id": "ada3cfb5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the emotion dataset by returning tokenized examples.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KIeah2n0FLA2"
      },
      "id": "KIeah2n0FLA2"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f95e5b0d",
      "metadata": {
        "id": "f95e5b0d",
        "outputId": "c4b68d54-354d-46ad-ffb0-73b001009fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e1c32cef00734b8d9c1073ac112179cd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1c32cef00734b8d9c1073ac112179cd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_data = {}\n",
        "\n",
        "for split in splits:\n",
        "    tokenized_data[split] = ds[split].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd364a8e",
      "metadata": {
        "id": "dd364a8e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
        "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43806846",
      "metadata": {
        "scrolled": true,
        "id": "43806846"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb59618",
      "metadata": {
        "id": "7cb59618"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbfe58a",
      "metadata": {
        "id": "dfbfe58a"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args= training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\"),\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f339ceb",
      "metadata": {
        "id": "2f339ceb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45571b43",
      "metadata": {
        "id": "45571b43"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcf55eb",
      "metadata": {
        "id": "8fcf55eb"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c757d11",
      "metadata": {
        "id": "3c757d11"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"gpt2_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "997171e1",
      "metadata": {
        "id": "997171e1"
      },
      "outputs": [],
      "source": [
        "gpt_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9c89f4",
      "metadata": {
        "id": "4e9c89f4"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    inference_mode=False,\n",
        "    r=18,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    modules_to_save=[\"classifier\"],\n",
        "    bias='lora_only',\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(gpt_model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f7721b",
      "metadata": {
        "id": "b0f7721b"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['c_attn', 'c_proj'],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    fan_in_fan_out=True,\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d89bbfd",
      "metadata": {
        "id": "7d89bbfd"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    label_names=[\"labels\"],\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95fa870e",
      "metadata": {
        "id": "95fa870e"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer = tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ac357f",
      "metadata": {
        "scrolled": false,
        "id": "d7ac357f"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39a8442",
      "metadata": {
        "id": "f39a8442"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1761bd",
      "metadata": {
        "id": "ad1761bd"
      },
      "outputs": [],
      "source": [
        "lora_model.save_pretrained(\"gpt-custom-lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e9add6",
      "metadata": {
        "id": "d7e9add6"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"gpt-custom-lora-tokenize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForSequenceClassification\n",
        "\n",
        "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt2_lora\")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2_lora_tokenizer\")"
      ],
      "metadata": {
        "id": "up5wN_e7nO01"
      },
      "id": "up5wN_e7nO01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70e99f5",
      "metadata": {
        "id": "b70e99f5"
      },
      "outputs": [],
      "source": [
        "# Define input text\n",
        "input_text = [\n",
        "    \"I absolutely loved the cinematography and the performances in this film. It's a masterpiece!\",\n",
        "    \"The plot was convoluted, and the acting felt forced. I wouldn't recommend this movie to anyone.\"\n",
        "]\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Convert logits to probabilities\n",
        "probs = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=1)\n",
        "\n",
        "# Map labels to sentiments\n",
        "sentiments = [\"Negative\", \"Positive\"]\n",
        "\n",
        "# Print results\n",
        "for i, text in enumerate(input_text):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {sentiments[predicted_labels[i].item()]} (Probability: {probs[i][predicted_labels[i]].item()})\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1c32cef00734b8d9c1073ac112179cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3d89f470e034f2ea0f33e155b495e7f",
              "IPY_MODEL_dd525127fe1d4c868bbd081188e7c7d5",
              "IPY_MODEL_c27d37b662174156969d5e1525027e53"
            ],
            "layout": "IPY_MODEL_2501a6bc6a454acf832bcbea048a9354"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}